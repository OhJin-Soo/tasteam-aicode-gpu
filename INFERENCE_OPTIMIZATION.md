# μ¶”λ΅  μ„±λ¥ μµμ ν™” κΈ°μ  ν‰κ°€ λ° μ μ© μ „λµ

**μ‘μ„±μΌ**: 2026λ…„ 1μ›”  
**ν”„λ΅μ νΈ**: Review Sentiment Analysis API  
**λ²„μ „**: 1.0.0

---

## π“‹ λ©μ°¨

1. [ν„μ¬ ν”„λ΅μ νΈ μƒν™©](#ν„μ¬-ν”„λ΅μ νΈ-μƒν™©)
2. [κΈ°μ λ³„ μƒμ„Έ ν‰κ°€](#κΈ°μ λ³„-μƒμ„Έ-ν‰κ°€)
3. [ν†µν•© μ μ© μ „λµ](#ν†µν•©-μ μ©-μ „λµ)
4. [κµ¬ν„ κ°€μ΄λ“](#κµ¬ν„-κ°€μ΄λ“)
5. [λΉ„μ©-ν¨κ³Ό λ¶„μ„](#λΉ„μ©-ν¨κ³Ό-λ¶„μ„)
6. [μµμΆ… κ¶μ¥μ‚¬ν•­](#μµμΆ…-κ¶μ¥μ‚¬ν•­)

---

## ν„μ¬ ν”„λ΅μ νΈ μƒν™©

### μ•„ν‚¤ν…μ² κ°μ”

- **κ°μ„± λ¶„μ„**: Transformers Pipeline (λ°°μΉ 32) β†’ LLM μ¬λ¶„λ¥ (ν™•μ‹ λ„ λ‚®μ€ κ²½μ°λ§)
- **μ„λ² λ”©**: SentenceTransformer (λ°°μΉ 32)
- **LLM μ¶”λ΅ **: OpenAI API (`gpt-4o-mini`)
- **λ²΅ν„° DB**: Qdrant (in-memory)

### μ„±λ¥ λ³‘λ© μ§€μ 

1. **Transformers Pipeline** (κ°μ„± λ¶„μ„)
   - CPU/GPU νΌμ© κ°€λ¥
   - λ°°μΉ μ²λ¦¬λ΅ μµμ ν™”λ¨ (λ°°μΉ ν¬κΈ°: 32)

2. **SentenceTransformer** (μ„λ² λ”©)
   - GPU κ°€μ† κ°€λ¥
   - λ°°μΉ μ²λ¦¬λ΅ μµμ ν™”λ¨ (λ°°μΉ ν¬κΈ°: 32)

3. **OpenAI API νΈμ¶** (LLM)
   - λ„¤νΈμ›ν¬ μ§€μ—°
   - λΉ„μ© λ°μƒ
   - λ™μ‹ μ”μ²­ μ ν•

4. **Qdrant λ²΅ν„° κ²€μƒ‰**
   - λ©”λ¨λ¦¬ κΈ°λ°
   - μƒλ€μ μΌλ΅ λΉ λ¦„

### LLM μ‚¬μ© ν¨ν„΄

- **κ°μ„± λ¶„μ„ μ¬λ¶„λ¥**: ν™•μ‹ λ„ < 0.8 λλ” ν‚¤μ›λ“("λ”λ°", "μ§€λ§") ν¬ν•¨ μ‹λ§ μ‚¬μ©
- **λ¦¬λ·° μ”μ•½**: λ²΅ν„° κ²€μƒ‰μΌλ΅ μ°Ύμ€ λ¦¬λ·° μ”μ•½
- **κ°•μ  μ¶”μ¶**: λ²΅ν„° κ²€μƒ‰μΌλ΅ μ°Ύμ€ λ¦¬λ·° λΉ„κµ λ¶„μ„

---

## κΈ°μ λ³„ μƒμ„Έ ν‰κ°€

### 1. μ§€μ‹μ¦λ¥ (Knowledge Distillation)

#### κ°λ…

ν° λ¨λΈ(Teacher)μ μ§€μ‹μ„ μ‘μ€ λ¨λΈ(Student)λ΅ μ „λ‹¬ν•μ—¬ μ„±λ¥μ„ μ μ§€ν•λ©΄μ„ λ¨λΈ ν¬κΈ°λ¥Ό μ¤„μ΄λ” κΈ°λ²•

#### μ μ© κ°€λ¥μ„±: β­β­β­ (μ¤‘κ°„)

#### μ¥μ 

1. **λ¨λΈ ν¬κΈ° κ°μ†**
   - Teacher: 7B λ¨λΈ β†’ Student: 1.5B-3B λ¨λΈ
   - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ°μ†
   - μ¶”λ΅  μ†λ„ ν–¥μƒ

2. **μ„±λ¥ μ μ§€**
   - Teacher λ¨λΈμ μ§€μ‹ μ „λ‹¬
   - λ„λ©”μΈ νΉν™” κ°€λ¥

3. **λΉ„μ© μ κ°**
   - μ‘μ€ λ¨λΈλ΅ μΈν• GPU μ”κµ¬μ‚¬ν•­ κ°μ†
   - μ„λΉ™ λΉ„μ© μ κ°

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **ν•™μµ λ³µμ΅λ„**
   - Teacher λ¨λΈ ν•„μ”
   - μ¦λ¥ ν•™μµ μ‹κ°„ μ†μ”
   - ν•μ΄νΌνλΌλ―Έν„° νλ‹ ν•„μ”

2. **μ„±λ¥ μ†μ‹¤ κ°€λ¥μ„±**
   - μ™„λ²½ν• μ§€μ‹ μ „λ‹¬ μ–΄λ ¤μ›€
   - λ³µμ΅ν• μ‘μ—…μ—μ„ μ„±λ¥ μ €ν• κ°€λ¥

3. **λ°μ΄ν„° μ”κµ¬μ‚¬ν•­**
   - μ¦λ¥μ© λ°μ΄ν„°μ…‹ ν•„μ”
   - Teacher λ¨λΈμ μμΈ΅ κ²°κ³Ό ν•„μ”

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# Teacher λ¨λΈ: Qwen2.5-7B-Instruct
# Student λ¨λΈ: Qwen2.5-1.5B-Instruct

# μ¦λ¥ κ³Όμ •
# 1. Teacher λ¨λΈλ΅ λ¦¬λ·° λ°μ΄ν„° μμΈ΅
# 2. Student λ¨λΈμ„ Teacherμ μμΈ΅κ³Ό μ‹¤μ  λ μ΄λΈ”λ΅ ν•™μµ
# 3. μ‘μ€ λ¨λΈλ΅ μ¶”λ΅  (μ†λ„ ν–¥μƒ)
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ°**: λΉ„κ¶μ¥ (λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤)
- **μ¤‘κΈ°**: vLLM λ„μ… ν›„ κ²€ν†  (λ¨λΈ ν¬κΈ° μµμ ν™” ν•„μ” μ‹)
- **μ¥κΈ°**: νΉμ • μ‘μ—…(κ°μ„± λ¶„μ„ μ¬λ¶„λ¥)μ—λ§ μ μ© κ²€ν† 

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ†’μ
- **μ„±λ¥ ν–¥μƒ**: μ¤‘κ°„ (2-3λ°° μ†λ„ ν–¥μƒ)
- **λΉ„μ© μ κ°**: μ¤‘κ°„
- **μΆ…ν•© ν‰κ°€**: β­β­β­ (μ¤‘κ°„)

---

### 2. μ–‘μν™” (Quantization)

#### κ°λ…

λ¨λΈμ κ°€μ¤‘μΉμ™€ ν™μ„±ν™” κ°’μ„ λ‚®μ€ λΉ„νΈλ΅ ν‘ν„ν•μ—¬ λ©”λ¨λ¦¬ μ‚¬μ©λ‰κ³Ό μ¶”λ΅  μ†λ„λ¥Ό κ°μ„ 

#### μ μ© κ°€λ¥μ„±: β­β­β­β­β­ (λ§¤μ° λ†’μ)

#### μ–‘μν™” μΆ…λ¥

1. **INT8 μ–‘μν™”**
   - FP32 β†’ INT8 λ³€ν™
   - λ©”λ¨λ¦¬ 4λ°° κ°μ†
   - μ†λ„ 2-4λ°° ν–¥μƒ

2. **FP16 μ–‘μν™”**
   - FP32 β†’ FP16 λ³€ν™
   - λ©”λ¨λ¦¬ 2λ°° κ°μ†
   - μ†λ„ 1.5-2λ°° ν–¥μƒ

3. **4-bit μ–‘μν™” (QLoRA)**
   - FP32 β†’ 4-bit λ³€ν™
   - λ©”λ¨λ¦¬ 8λ°° κ°μ†
   - μ†λ„ 3-5λ°° ν–¥μƒ

#### μ¥μ 

1. **λ©”λ¨λ¦¬ ν¨μ¨μ„±**
   - λ¨λΈ ν¬κΈ° λ€ν­ κ°μ†
   - λ” ν° λ°°μΉ ν¬κΈ° κ°€λ¥
   - GPU λ©”λ¨λ¦¬ μ μ•½

2. **μ¶”λ΅  μ†λ„ ν–¥μƒ**
   - λ‚®μ€ λΉ„νΈ μ—°μ‚°μΌλ΅ μ†λ„ ν–¥μƒ
   - GPU ν™μ©λ¥  μ¦κ°€

3. **λΉ„μ© μ κ°**
   - μ‘μ€ GPUλ΅λ„ λ€ν• λ¨λΈ μ‹¤ν–‰ κ°€λ¥
   - μ„λΉ™ λΉ„μ© μ κ°

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **μ„±λ¥ μ†μ‹¤ κ°€λ¥μ„±**
   - μ–‘μν™”λ΅ μΈν• μ •ν™•λ„ μ €ν•
   - λ³µμ΅ν• μ‘μ—…μ—μ„ μν–¥ νΌ

2. **μ–‘μν™” λ³µμ΅λ„**
   - λ¨λΈλ³„ μµμ  μ–‘μν™” λ°©λ²• λ‹¤λ¦„
   - μΊλ¦¬λΈλ μ΄μ… ν•„μ”

3. **ν•λ“μ›¨μ–΄ μ§€μ›**
   - INT8μ€ μµμ‹  GPUμ—μ„λ§ μµμ ν™”
   - TensorRTμ™€ ν•¨κ» μ‚¬μ© μ‹ ν¨κ³Όμ 

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# 1. Transformers Pipeline μ–‘μν™”
from transformers import pipeline
import torch

# FP16 μ–‘μν™”
model = pipeline(
    "sentiment-analysis",
    model="Dilwolf/Kakao_app-kr_sentiment",
    device=0,
    torch_dtype=torch.float16,  # FP16 μ–‘μν™”
)

# 2. SentenceTransformer μ–‘μν™”
from sentence_transformers import SentenceTransformer

encoder = SentenceTransformer("jhgan/ko-sbert-multitask")
encoder = encoder.half()  # FP16 λ³€ν™

# 3. LLM μ–‘μν™” (4-bit)
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
```

#### κ¶μ¥μ‚¬ν•­

- **μ¦‰μ‹ μ μ©**: FP16 μ–‘μν™” (Transformers, SentenceTransformer)
- **λ‹¨κΈ° μ μ©**: INT8 μ–‘μν™” (TensorRTμ™€ ν•¨κ»)
- **μ¤‘κΈ° μ μ©**: 4-bit μ–‘μν™” (LLM λ¨λΈ)

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ‚®μ-μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: λ†’μ (2-5λ°° μ†λ„ ν–¥μƒ)
- **λΉ„μ© μ κ°**: λ†’μ
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­β­ (λ§¤μ° λ†’μ)

---

### 3. LoRA (Low-Rank Adaptation)

#### κ°λ…

λ¨λΈμ κ°€μ¤‘μΉλ¥Ό μ§μ ‘ μμ •ν•μ§€ μ•κ³ , λ‚®μ€ λ­ν¬ ν–‰λ ¬μ„ μ¶”κ°€ν•μ—¬ νμΈνλ‹ν•λ” ν¨μ¨μ  λ°©λ²•

#### μ μ© κ°€λ¥μ„±: β­β­β­β­ (λ†’μ)

#### μ¥μ 

1. **λ©”λ¨λ¦¬ ν¨μ¨μ„±**
   - μ „μ²΄ λ¨λΈ νμΈνλ‹ λ€λΉ„ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ†
   - μ‘μ€ GPUλ΅λ„ λ€ν• λ¨λΈ νμΈνλ‹ κ°€λ¥

2. **ν•™μµ μ†λ„**
   - ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°κ°€ μ μ–΄ ν•™μµ μ†λ„ λΉ λ¦„
   - μ—¬λ¬ μ‘μ—…μ— λ€ν• LoRA μ–΄λ‘ν„° κ³µμ  κ°€λ¥

3. **μ μ—°μ„±**
   - μ‘μ—…λ³„ LoRA μ–΄λ‘ν„° μƒμ„± κ°€λ¥
   - Base λ¨λΈμ€ κ³µμ ν•κ³  μ–΄λ‘ν„°λ§ κµμ²΄

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **λ°μ΄ν„°μ…‹ ν•„μ”**
   - νμΈνλ‹μ© λ°μ΄ν„°μ…‹ κµ¬μ¶• ν•„μ”
   - λ μ΄λΈ”λ§ ν’μ§μ΄ μ¤‘μ”

2. **ν•™μµ λΉ„μ©**
   - GPU λ¦¬μ†μ¤ ν•„μ”
   - ν•™μµ μ‹κ°„ μ†μ”

3. **μ„±λ¥ μ ν•**
   - μ™„μ „ νμΈνλ‹ λ€λΉ„ μ„±λ¥ μ ν• κ°€λ¥
   - λ³µμ΅ν• μ‘μ—…μ—μ„ ν¨κ³Ό μ ν•μ 

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# LoRA νμΈνλ‹ μμ‹
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# Base λ¨λΈ λ΅λ“
model = AutoModelForCausalLM.from_pretrained("Qwen2.5-7B-Instruct")

# LoRA μ„¤μ •
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)

# ν•κµ­μ–΄ λ¦¬λ·° λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹
# - κ°μ„± λ¶„μ„ μ¬λ¶„λ¥
# - λ¦¬λ·° μ”μ•½
# - κ°•μ  μ¶”μ¶
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ°**: λΉ„κ¶μ¥ (λ°μ΄ν„°μ…‹ κµ¬μ¶• ν•„μ”)
- **μ¤‘κΈ°**: λ°μ΄ν„°μ…‹ ν™•λ³΄ ν›„ κ²€ν† 
- **μ¥κΈ°**: λ„λ©”μΈ νΉν™” ν•„μ” μ‹ μ μ©

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: μ¤‘κ°„-λ†’μ (λ„λ©”μΈ νΉν™” μ‹)
- **λΉ„μ© μ κ°**: μ¤‘κ°„
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­ (λ†’μ, λ°μ΄ν„°μ…‹ ν™•λ³΄ ν›„)

---

### 4. QLoRA (Quantized LoRA)

#### κ°λ…

4-bit μ–‘μν™”μ™€ LoRAλ¥Ό κ²°ν•©ν•μ—¬ λ©”λ¨λ¦¬ ν¨μ¨μ„±μ„ κ·Ήλ€ν™”ν• νμΈνλ‹ λ°©λ²•

#### μ μ© κ°€λ¥μ„±: β­β­β­β­ (λ†’μ)

#### μ¥μ 

1. **κ·Ήλ„μ λ©”λ¨λ¦¬ ν¨μ¨μ„±**
   - 4-bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ 8λ°° κ°μ†
   - LoRAλ΅ ν•™μµ νλΌλ―Έν„° μµμ†ν™”
   - RTX 3090 (24GB)λ΅λ„ 7B λ¨λΈ νμΈνλ‹ κ°€λ¥

2. **λΉ„μ© ν¨μ¨μ„±**
   - μ‘μ€ GPUλ΅λ„ λ€ν• λ¨λΈ νμΈνλ‹
   - ν•™μµ λΉ„μ© μ κ°

3. **μ„±λ¥ μ μ§€**
   - μ–‘μν™” + LoRA μ΅°ν•©μΌλ΅ μ„±λ¥ μ μ§€
   - λ„λ©”μΈ νΉν™” κ°€λ¥

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **λ°μ΄ν„°μ…‹ κµ¬μ¶• ν•„μ”**
   - μµμ† 5,000-10,000κ° μƒν” κ¶μ¥
   - κ° μ‘μ—…λ³„ λ°μ΄ν„°μ…‹ ν•„μ”
   - λ μ΄λΈ”λ§ ν’μ§ μ¤‘μ”

2. **ν•™μµ μ‹κ°„**
   - λ¨λΈ ν¬κΈ°μ— λ”°λΌ μμ‹κ°„~μμΌ μ†μ”
   - μ‹¤ν— λ° κ²€μ¦ μ‹κ°„ ν•„μ”

3. **μ„±λ¥ μ†μ‹¤ κ°€λ¥μ„±**
   - 4-bit μ–‘μν™”λ΅ μΈν• λ―Έμ„Έν• μ„±λ¥ μ €ν•
   - λ³µμ΅ν• μ‘μ—…μ—μ„ μν–¥ κ°€λ¥

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# QLoRA νμΈνλ‹ μμ‹
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 4-bit μ–‘μν™” μ„¤μ •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Base λ¨λΈ λ΅λ“
model = AutoModelForCausalLM.from_pretrained(
    "Qwen2.5-7B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
)

# LoRA μ„¤μ •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# ν•κµ­μ–΄ λ¦¬λ·° λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ°**: λΉ„κ¶μ¥ (λ°μ΄ν„°μ…‹ κµ¬μ¶• ν•„μ”)
- **μ¤‘κΈ°**: λ°μ΄ν„°μ…‹ ν™•λ³΄ ν›„ κ²€ν†  (κ°€μ¥ μ‹¤μ©μ )
- **μ¥κΈ°**: λ„λ©”μΈ νΉν™” ν•„μ” μ‹ μ μ©

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: μ¤‘κ°„-λ†’μ (λ„λ©”μΈ νΉν™” μ‹)
- **λΉ„μ© μ κ°**: λ†’μ (λ©”λ¨λ¦¬ ν¨μ¨μ„±)
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­ (λ†’μ, λ°μ΄ν„°μ…‹ ν™•λ³΄ ν›„)

---

### 5. MoE (Mixture of Experts) νμΈνλ‹

#### κ°λ…

μ—¬λ¬ μ „λ¬Έκ°€(Expert) λ¨λΈμ„ μ΅°ν•©ν•μ—¬ κ° μ‘μ—…μ— μµμ ν™”λ μ „λ¬Έκ°€λ¥Ό λ™μ μΌλ΅ μ„ νƒν•λ” μ•„ν‚¤ν…μ²

#### μ μ© κ°€λ¥μ„±: β­β­ (λ‚®μ)

#### MoE μ•„ν‚¤ν…μ² κ°μ”

**κΈ°λ³Έ κµ¬μ΅°:**
- **Router**: μ…λ ¥μ— λ”°λΌ μ μ ν• μ „λ¬Έκ°€ μ„ νƒ
- **Experts**: κ°κ° λ‹¤λ¥Έ μ‘μ—…μ— νΉν™”λ λ¨λΈλ“¤
  - Expert 1: κ°μ„± λ¶„μ„ μ¬λ¶„λ¥
  - Expert 2: λ¦¬λ·° μ”μ•½
  - Expert 3: κ°•μ  μ¶”μ¶
- **Gating Network**: μ „λ¬Έκ°€ μ„ νƒμ„ μ„ν• λΌμ°ν… λ„¤νΈμ›ν¬

#### μ¥μ 

1. **ν¨μ¨μ μΈ μ¶”λ΅ **
   - ν™μ„±ν™”λλ” μ „λ¬Έκ°€λ§ μ‚¬μ© (Sparse Activation)
   - μ „μ²΄ νλΌλ―Έν„° λ€λΉ„ μ‹¤μ  μ‚¬μ© νλΌλ―Έν„° μ μ
   - λ©”λ¨λ¦¬ ν¨μ¨μ 

2. **μ „λ¬Έμ„± λ¶„λ¦¬**
   - κ° μ‘μ—…λ³„λ΅ μµμ ν™”λ μ „λ¬Έκ°€ λ¨λΈ
   - μ‘μ—… κ°„ κ°„μ„­ μµμ†ν™”
   - νΉμ • μ‘μ—… μ„±λ¥ ν–¥μƒ κ°€λ¥

3. **ν™•μ¥μ„±**
   - μƒλ΅μ΄ μ‘μ—… μ¶”κ°€ μ‹ μƒλ΅μ΄ μ „λ¬Έκ°€ μ¶”κ°€ κ°€λ¥
   - κΈ°μ΅΄ μ „λ¬Έκ°€λ” κ·Έλ€λ΅ μ μ§€

4. **λ©€ν‹°νƒμ¤ν¬ ν•™μµ**
   - μ—¬λ¬ μ‘μ—…μ„ λ™μ‹μ— ν•™μµ
   - κ³µν†µ μ§€μ‹ κ³µμ  κ°€λ¥

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **νμΈνλ‹ λ³µμ΅λ„**
   - MoE λ¨λΈ νμΈνλ‹μ€ ν‘μ¤€ λ¨λΈλ³΄λ‹¤ λ§¤μ° λ³µμ΅
   - μ „λ¬Έκ°€ λΌμ°ν… ν•™μµ ν•„μ”
   - Gating Network ν•™μµ ν•„μ”
   - λ°μ΄ν„°μ…‹ κµ¬μ„±κ³Ό ν•™μµ μ „λµ μ„¤κ³„ λ³µμ΅

2. **λ°μ΄ν„° μ”κµ¬μ‚¬ν•­**
   - ν•κµ­μ–΄ λ¦¬λ·° λ°μ΄ν„°μ…‹ ν•„μ”
   - κ° μ‘μ—…λ³„ λ μ΄λΈ”λ§λ λ°μ΄ν„° ν•„μ”
   - μ¶©λ¶„ν• μ–‘μ λ°μ΄ν„° (μλ§~μμ‹­λ§ κ±΄)
   - λ°μ΄ν„° λ¶„ν¬ κ· ν• μ¤‘μ”

3. **ν•™μµ λΉ„μ©**
   - GPU λ¦¬μ†μ¤ (ν•™μµμ©, λ€κ·λ¨ ν•„μ”)
   - ν•™μµ μ‹κ°„ (ν‘μ¤€ λ¨λΈ λ€λΉ„ 2-3λ°°)
   - μ‹¤ν— λ° κ²€μ¦ μ‹κ°„
   - ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ³µμ΅

4. **λΌμ°ν… λ¶μ•μ •μ„±**
   - μ „λ¬Έκ°€ μ„ νƒμ΄ λ¶μ•μ •ν•  μ μμ
   - νΉμ • μ „λ¬Έκ°€μ— νΈν–¥λ  μ μμ
   - λΌμ°ν… ν•™μµμ΄ μ–΄λ ¤μ›€

5. **μ„λΉ™ λ³µμ΅λ„**
   - μ—¬λ¬ μ „λ¬Έκ°€ λ¨λΈ κ΄€λ¦¬ ν•„μ”
   - λΌμ°ν… λ΅μ§ κµ¬ν„ ν•„μ”
   - λ©”λ¨λ¦¬ κ΄€λ¦¬ λ³µμ΅

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# MoE νμΈνλ‹ μμ‹
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

# Base λ¨λΈ (μ: Mixtral 8x7B λλ” μ»¤μ¤ν…€ MoE)
# κ° μ „λ¬Έκ°€λ” LoRA μ–΄λ‘ν„°λ΅ κµ¬ν„ κ°€λ¥

# Expert 1: κ°μ„± λ¶„μ„ μ¬λ¶„λ¥
expert1_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
)
expert1_model = get_peft_model(base_model, expert1_config)

# Expert 2: λ¦¬λ·° μ”μ•½
expert2_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
)
expert2_model = get_peft_model(base_model, expert2_config)

# Expert 3: κ°•μ  μ¶”μ¶
expert3_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
)
expert3_model = get_peft_model(base_model, expert3_config)

# Gating Network ν•™μµ
# - μ…λ ¥μ— λ”°λΌ μ μ ν• μ „λ¬Έκ°€ μ„ νƒ
# - λΌμ°ν… λ΅μ§ ν•™μµ

# μ¶”λ΅  μ‹
def route_to_expert(input_text, task_type):
    """μ‘μ—… νƒ€μ…μ— λ”°λΌ μ „λ¬Έκ°€ μ„ νƒ"""
    if task_type == "sentiment":
        return expert1_model
    elif task_type == "summarize":
        return expert2_model
    elif task_type == "strengths":
        return expert3_model
```

#### MoE vs λ‹¨μΌ λ¨λΈ λΉ„κµ

| ν•­λ© | λ‹¨μΌ λ¨λΈ | MoE λ¨λΈ |
|------|----------|---------|
| **ν•™μµ λ³µμ΅λ„** | λ‚®μ | λ§¤μ° λ†’μ |
| **μ¶”λ΅  ν¨μ¨μ„±** | μ¤‘κ°„ | λ†’μ (Sparse Activation) |
| **λ©”λ¨λ¦¬ μ‚¬μ©** | λ†’μ | μ¤‘κ°„ (ν™μ„± μ „λ¬Έκ°€λ§) |
| **μ‘μ—…λ³„ μ„±λ¥** | μ¤‘κ°„ | λ†’μ (μ „λ¬Έκ°€ νΉν™”) |
| **ν™•μ¥μ„±** | λ‚®μ | λ†’μ |
| **μ„λΉ™ λ³µμ΅λ„** | λ‚®μ | λ†’μ |

#### λ€μ•: μ‘μ—…λ³„ λ³„λ„ λ¨λΈ (κ¶μ¥)

ν„μ¬ ν”„λ΅μ νΈμ—λ” MoE λ€μ‹  **μ‘μ—…λ³„ λ³„λ„ λ¨λΈ**μ΄ λ” μ‹¤μ©μ :

```python
# μ‘μ—…λ³„ λ³„λ„ λ¨λΈ (λ” μ‹¤μ©μ )
# - κ°μ„± λ¶„μ„ λ¨λΈ: QLoRA νμΈνλ‹
# - μ”μ•½ λ¨λΈ: QLoRA νμΈνλ‹
# - κ°•μ  μ¶”μ¶ λ¨λΈ: QLoRA νμΈνλ‹

# κ° λ¨λΈμ„ λ…λ¦½μ μΌλ΅ κ΄€λ¦¬
# - ν•™μµμ΄ κ°„λ‹¨ν•¨
- μ„λΉ™μ΄ κ°„λ‹¨ν•¨
# - λ””λ²„κΉ…μ΄ μ‰¬μ›€
# - λ¨λΈ κµμ²΄κ°€ μ©μ΄ν•¨
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ°**: MoE νμΈνλ‹ λΉ„κ¶μ¥
  - ν„μ¬ ν•μ΄λΈλ¦¬λ“ μ ‘κ·Όμ΄ ν¨κ³Όμ 
  - νμΈνλ‹ ROIκ°€ λ§¤μ° λ¶ν™•μ‹¤
  - λ°μ΄ν„°μ…‹ κµ¬μ¶• λΉ„μ©μ΄ νΌ
  - ν•™μµ λ³µμ΅λ„κ°€ λ§¤μ° λ†’μ

- **μ¤‘κΈ°**: μ‘μ—…λ³„ λ³„λ„ λ¨λΈ κ²€ν† 
  - κ° μ‘μ—…μ— μµμ ν™”λ λ¨λΈ κµ¬μ¶•
  - MoEλ³΄λ‹¤ μ‹¤μ©μ μ΄κ³  κ΄€λ¦¬ μ©μ΄

- **μ¥κΈ°**: λ§¤μ° νΉμν• μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§ MoE κ²€ν† 
  - λ©€ν‹°νƒμ¤ν¬ ν•™μµμ΄ ν•„μμ μΌ λ•
  - λ©”λ¨λ¦¬ μ μ•½μ΄ λ§¤μ° μ‹¬ν•  λ•
  - μ¶©λ¶„ν• λ°μ΄ν„°μ™€ λ¦¬μ†μ¤κ°€ ν™•λ³΄λ κ²½μ°
  - MoE νμΈνλ‹ μ „λ¬Έ μ§€μ‹μ΄ μλ” κ²½μ°

#### MoE μ μ© μ΅°κ±΄

MoE νμΈνλ‹μ„ κ³ λ ¤ν•  μ μλ” μ΅°κ±΄:

1. β… **μ¶©λ¶„ν• λ°μ΄ν„°**
   - κ° μ‘μ—…λ³„ μµμ† 10,000κ° μ΄μƒ μƒν”
   - λ°μ΄ν„° λ¶„ν¬ κ· ν•
   - κ³ ν’μ§ λ μ΄λΈ”λ§

2. β… **μ¶©λ¶„ν• λ¦¬μ†μ¤**
   - λ€κ·λ¨ GPU (A100 80GB μ΄μƒ)
   - ν•™μµ μ‹κ°„ (μμ£Ό~μκ°μ›”)
   - MoE νμΈνλ‹ μ „λ¬Έ μ§€μ‹

3. β… **λ…ν™•ν• ROI**
   - λ‹¨μΌ λ¨λΈλ΅ λ©ν‘ μ„±λ¥ λ‹¬μ„± λ¶κ°€λ¥
   - λ©”λ¨λ¦¬ μ μ•½μ΄ λ§¤μ° μ‹¬ν•¨
   - λ©€ν‹°νƒμ¤ν¬ ν•™μµμ΄ ν•„μμ 

4. β… **μ¥κΈ° μ΄μ κ³„ν**
   - ν”„λ΅λ•μ… ν™κ²½μ—μ„ μ¥κΈ° μ΄μ
   - μ§€μ†μ μΈ λ¨λΈ κ°μ„  κ³„ν

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ§¤μ° λ†’μ
- **μ„±λ¥ ν–¥μƒ**: λ¶ν™•μ‹¤ (μ‘μ—…λ³„ λ³„λ„ λ¨λΈκ³Ό μ μ‚¬ν•  μ μμ)
- **λΉ„μ© μ κ°**: λ‚®μ (ν•™μµ λΉ„μ©μ΄ νΌ)
- **μΆ…ν•© ν‰κ°€**: β­β­ (λ‚®μ, λ§¤μ° νΉμν• κ²½μ°μ—λ§)

#### κ²°λ΅ 

**ν„μ¬ ν”„λ΅μ νΈμ—λ” MoE νμΈνλ‹μ„ κ¶μ¥ν•μ§€ μ•μµλ‹λ‹¤.**

**λ€μ‹  κ¶μ¥ν•λ” μ ‘κ·Ό:**
1. **μ‘μ—…λ³„ λ³„λ„ λ¨λΈ** (QLoRA νμΈνλ‹)
   - κ°μ„± λ¶„μ„ λ¨λΈ
   - μ”μ•½ λ¨λΈ
   - κ°•μ  μ¶”μ¶ λ¨λΈ
   - κ°κ° λ…λ¦½μ μΌλ΅ ν•™μµ λ° μ„λΉ™

2. **λ‹¨κ³„μ  μ μ©**
   - λ¨Όμ € λ‹¨μΌ μ‘μ—…λ¶€ν„° νμΈνλ‹
   - μ„±λ¥ κ²€μ¦ ν›„ λ‹¤λ¥Έ μ‘μ—… ν™•μ¥
   - ν•„μ” μ‹ ν†µν•© λ¨λΈ κ²€ν† 

3. **MoEλ” μµν›„μ μλ‹¨**
   - λ¨λ“  λ‹¤λ¥Έ λ°©λ²•μ΄ μ‹¤ν¨ν–μ„ λ•λ§ κ²€ν† 
   - λ§¤μ° νΉμν• μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§

---

### 6. λ°°μΉ μ²λ¦¬ λ° μΊμ‹±

#### κ°λ…

μ—¬λ¬ μ”μ²­μ„ λ¬¶μ–΄μ„ μ²λ¦¬ν•κ³ , λ™μΌν• μ…λ ¥μ— λ€ν• κ²°κ³Όλ¥Ό μΊμ‹±ν•μ—¬ μ¬μ‚¬μ©

#### μ μ© κ°€λ¥μ„±: β­β­β­β­β­ (λ§¤μ° λ†’μ)

#### λ°°μΉ μ²λ¦¬

**ν„μ¬ μƒνƒ:**
- Transformers Pipeline: λ°°μΉ 32
- SentenceTransformer: λ°°μΉ 32

**μµμ ν™” λ°©μ•:**

```python
# λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ •
import torch

def get_optimal_batch_size(model, device, max_batch_size=128):
    """GPU λ©”λ¨λ¦¬μ— λ§λ” μµμ  λ°°μΉ ν¬κΈ° κ³„μ‚°"""
    if device == -1:  # CPU
        return 32
    
    # GPU λ©”λ¨λ¦¬ ν™•μΈ
    gpu_memory = torch.cuda.get_device_properties(device).total_memory
    gpu_memory_gb = gpu_memory / (1024**3)
    
    if gpu_memory_gb >= 24:  # A100, RTX 3090
        return max_batch_size
    elif gpu_memory_gb >= 16:  # RTX 4080
        return 64
    elif gpu_memory_gb >= 12:  # RTX 3060
        return 32
    else:
        return 16

# μ μ©
batch_size = get_optimal_batch_size(self.sentiment, device)
```

#### μΊμ‹± μ „λµ

**1. κ²°κ³Ό μΊμ‹± (Response Caching)**

```python
# Redisλ¥Ό μ‚¬μ©ν• κ²°κ³Ό μΊμ‹±
import redis
import hashlib
import json

class CachedLLMUtils:
    def __init__(self, llm_utils, redis_client=None):
        self.llm_utils = llm_utils
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.cache_ttl = 3600  # 1μ‹κ°„
    
    def _get_cache_key(self, prompt, task_type):
        """μΊμ‹ ν‚¤ μƒμ„±"""
        content = f"{task_type}:{prompt}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def summarize_reviews(self, positive_reviews, negative_reviews):
        # μΊμ‹ ν‚¤ μƒμ„±
        prompt = json.dumps({
            "positive": positive_reviews,
            "negative": negative_reviews
        }, sort_keys=True)
        cache_key = self._get_cache_key(prompt, "summarize")
        
        # μΊμ‹ ν™•μΈ
        cached_result = self.redis.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        # LLM νΈμ¶
        result = self.llm_utils.summarize_reviews(positive_reviews, negative_reviews)
        
        # μΊμ‹ μ €μ¥
        self.redis.setex(cache_key, self.cache_ttl, json.dumps(result))
        
        return result
```

**2. μ„λ² λ”© μΊμ‹±**

```python
# λ²΅ν„° μ„λ² λ”© μΊμ‹±
class CachedVectorSearch:
    def __init__(self, vector_search, redis_client=None):
        self.vector_search = vector_search
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.cache_ttl = 86400  # 24μ‹κ°„
    
    def encode_with_cache(self, text):
        """μ„λ² λ”© μΊμ‹±"""
        cache_key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"
        
        # μΊμ‹ ν™•μΈ
        cached_embedding = self.redis.get(cache_key)
        if cached_embedding:
            return json.loads(cached_embedding)
        
        # μ„λ² λ”© μƒμ„±
        embedding = self.vector_search.encoder.encode(text)
        
        # μΊμ‹ μ €μ¥
        self.redis.setex(cache_key, self.cache_ttl, json.dumps(embedding.tolist()))
        
        return embedding
```

**3. κ°μ„± λ¶„μ„ κ²°κ³Ό μΊμ‹±**

```python
# κ°μ„± λ¶„μ„ κ²°κ³Ό μΊμ‹±
class CachedSentimentAnalyzer:
    def __init__(self, sentiment_analyzer, redis_client=None):
        self.analyzer = sentiment_analyzer
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.cache_ttl = 86400  # 24μ‹κ°„
    
    def analyze_with_cache(self, review_text):
        """κ°μ„± λ¶„μ„ κ²°κ³Ό μΊμ‹±"""
        cache_key = f"sentiment:{hashlib.md5(review_text.encode()).hexdigest()}"
        
        # μΊμ‹ ν™•μΈ
        cached_result = self.redis.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        # κ°μ„± λ¶„μ„ μν–‰
        result = self.analyzer.sentiment(review_text)[0]
        
        # μΊμ‹ μ €μ¥
        self.redis.setex(cache_key, self.cache_ttl, json.dumps(result))
        
        return result
```

#### μ¥μ 

1. **μ„±λ¥ ν–¥μƒ**
   - λ°°μΉ μ²λ¦¬λ΅ μ²λ¦¬λ‰ μ¦κ°€
   - μΊμ‹±μΌλ΅ λ°λ³µ μ”μ²­ μ²λ¦¬ μ‹κ°„ λ‹¨μ¶•

2. **λΉ„μ© μ κ°**
   - LLM API νΈμ¶ κ°μ†
   - GPU μ‚¬μ©λ‰ μµμ ν™”

3. **μ‚¬μ©μ κ²½ν—**
   - μ‘λ‹µ μ‹κ°„ λ‹¨μ¶•
   - μΌκ΄€λ κ²°κ³Ό μ κ³µ

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **λ©”λ¨λ¦¬ μ‚¬μ©**
   - μΊμ‹ μ €μ¥ κ³µκ°„ ν•„μ”
   - Redis λ©”λ¨λ¦¬ κ΄€λ¦¬ ν•„μ”

2. **μΊμ‹ λ¬΄ν¨ν™”**
   - λ°μ΄ν„° μ—…λ°μ΄νΈ μ‹ μΊμ‹ λ¬΄ν¨ν™” ν•„μ”
   - TTL μ„¤μ • μ¤‘μ”

3. **λ°°μΉ μ§€μ—°**
   - λ°°μΉκ°€ μ±„μ›μ§ λ•κΉμ§€ λ€κΈ° μ‹κ°„ λ°μƒ
   - λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ • ν•„μ”

#### κ¶μ¥μ‚¬ν•­

- **μ¦‰μ‹ μ μ©**: κ²°κ³Ό μΊμ‹± (Redis)
- **λ‹¨κΈ° μ μ©**: λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ •
- **μ¤‘κΈ° μ μ©**: κ³ κΈ‰ μΊμ‹± μ „λµ (λ¶€λ¶„ μΊμ‹±, κ³„μΈµμ  μΊμ‹±)

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ‚®μ-μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: λ§¤μ° λ†’μ (5-10λ°° μ†λ„ ν–¥μƒ κ°€λ¥)
- **λΉ„μ© μ κ°**: λ†’μ (API νΈμ¶ κ°μ†)
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­β­ (λ§¤μ° λ†’μ)

---

### 7. vLLM (Very Large Language Model Inference)

#### κ°λ…

λ€κ·λ¨ μ–Έμ–΄ λ¨λΈμ κ³ μ„±λ¥ μ¶”λ΅ μ„ μ„ν• μ„λΉ™ ν”„λ μ„μ›ν¬ (PagedAttention κΈ°λ°)

#### μ μ© κ°€λ¥μ„±: β­β­β­β­β­ (λ§¤μ° λ†’μ)

#### μ¥μ 

1. **μ¶”λ΅  μ†λ„ ν–¥μƒ**
   - PagedAttentionμΌλ΅ μ²λ¦¬λ‰ μ¦κ°€
   - λ™μ‹ μ”μ²­ μ²λ¦¬ ν¨μ¨ ν–¥μƒ
   - OpenAI API λ€λΉ„ μ§€μ—° μ‹κ°„ κ°μ†

2. **λΉ„μ© μ κ°**
   - μμ²΄ νΈμ¤ν…μΌλ΅ ν† ν° κΈ°λ° λΉ„μ© μ κ±°
   - νΈλν”½ μ¦κ°€ μ‹ λΉ„μ© μ κ° ν¨κ³Ό νΌ

3. **ν”„λΌμ΄λ²„μ‹/λ³΄μ•**
   - λ°μ΄ν„°κ°€ μ™Έλ¶€λ΅ μ „μ†΅λμ§€ μ•μ
   - κ·μ  μ¤€μ μ©μ΄

4. **μ»¤μ¤ν„°λ§μ΄μ§•**
   - λ¨λΈ μ„ νƒ μμ λ„
   - νμΈνλ‹ λ¨λΈ μ‚¬μ© κ°€λ¥

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **μΈν”„λΌ λ³µμ΅λ„**
   - GPU μ„λ²„ ν•„μ” (A100/H100 λλ” 2x RTX 4090)
   - λ¨λΈ μ„λΉ™ μΈν”„λΌ κµ¬μ¶• ν•„μ”
   - λ¨λ‹ν„°λ§, λ΅κΉ…, μ¤μΌ€μΌλ§ κ΄€λ¦¬ ν•„μ”

2. **λ¨λΈ μ„ νƒ**
   - ν•κµ­μ–΄ μ„±λ¥μ΄ μΆ‹μ€ λ¨λΈ ν•„μ”
   - μ¶”μ²: Qwen2.5-7B-Instruct, SOLAR-Korean-Instruct

3. **μ΄κΈ° ν¬μ**
   - GPU μΈν”„λΌ λΉ„μ©
   - DevOps μΈλ ¥ ν•„μ”

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# vLLM μ„λ²„ μ„¤μ •
from vllm import LLM, SamplingParams

# λ¨λΈ λ΅λ“
llm = LLM(
    model="Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

sampling_params = SamplingParams(
    temperature=0.3,
    top_p=0.95,
    max_tokens=1024,
)

# API μ„λ²„ (FastAPI)
from fastapi import FastAPI
app = FastAPI()

@app.post("/generate")
async def generate(prompt: str):
    outputs = llm.generate([prompt], sampling_params)
    return {"text": outputs[0].outputs[0].text}
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ° μ μ©**: νΈλν”½ μ¦κ°€ μ‹ (μ›” API λΉ„μ© $1,000 μ΄μƒ)
- **μ¤‘κΈ° μ μ©**: ν”„λΌμ΄λ²„μ‹ μ”κµ¬μ‚¬ν•­ λ°μƒ μ‹
- **μ¥κΈ° μ μ©**: μ»¤μ¤ν„°λ§μ΄μ§• ν•„μ” μ‹

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: λ§¤μ° λ†’μ (5-10λ°° μ†λ„ ν–¥μƒ)
- **λΉ„μ© μ κ°**: λ†’μ (API λΉ„μ© μ κ±°)
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­β­ (λ§¤μ° λ†’μ, νΈλν”½ μ¦κ°€ μ‹)

---

### 8. CUDA μµμ ν™”

#### κ°λ…

GPUλ¥Ό ν™μ©ν• λ³‘λ ¬ μ²λ¦¬λ΅ μ¶”λ΅  μ†λ„ ν–¥μƒ

#### μ μ© κ°€λ¥μ„±: β­β­β­β­β­ (λ§¤μ° λ†’μ)

#### μ¥μ 

1. **μ¦‰μ‹ μ μ© κ°€λ¥**
   - μ½”λ“ λ³€κ²½ μµμ†ν™”
   - κΈ°μ΅΄ λ¨λΈ κ·Έλ€λ΅ μ‚¬μ©
   - GPUλ§ μμΌλ©΄ μ μ© κ°€λ¥

2. **μ„±λ¥ ν–¥μƒ**
   - Transformers: 2-5λ°° μ†λ„ ν–¥μƒ
   - SentenceTransformer: 3-10λ°° μ†λ„ ν–¥μƒ
   - λ°°μΉ ν¬κΈ° μ¦κ°€ κ°€λ¥

3. **λΉ„μ© ν¨μ¨**
   - μ¶”κ°€ μΈν”„λΌ λ¶ν•„μ”
   - κΈ°μ΅΄ GPU ν™μ©

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **GPU ν•„μ”**
   - μµμ† RTX 3060 μ΄μƒ
   - VRAM μ©λ‰μ— λ”°λΌ λ°°μΉ ν¬κΈ° μ ν•

2. **λ©”λ¨λ¦¬ κ΄€λ¦¬**
   - GPU λ©”λ¨λ¦¬ λ¶€μ΅± μ‹ OOM μ—λ¬
   - λ°°μΉ ν¬κΈ° λ™μ  μ΅°μ • ν•„μ”

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# Transformers Pipeline CUDA μµμ ν™”
import torch

device = 0 if torch.cuda.is_available() else -1
self.sentiment = pipeline(
    "sentiment-analysis",
    model=model_name,
    tokenizer=model_name,
    device=device,
    batch_size=64 if device >= 0 else 32,
)

# SentenceTransformer CUDA μµμ ν™”
encoder = SentenceTransformer("jhgan/ko-sbert-multitask")
if torch.cuda.is_available():
    encoder = encoder.cuda()
    batch_size = 64
else:
    batch_size = 32
```

#### κ¶μ¥μ‚¬ν•­

- **μ¦‰μ‹ μ μ©**: GPUκ°€ μλ” κ²½μ° ν•„μ
- **λ‹¨κΈ° μ μ©**: GPU μΈν”„λΌ κµ¬μ¶•

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ§¤μ° λ‚®μ
- **μ„±λ¥ ν–¥μƒ**: λ†’μ (2-10λ°° μ†λ„ ν–¥μƒ)
- **λΉ„μ© μ κ°**: μ¤‘κ°„
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­β­ (λ§¤μ° λ†’μ)

---

### 9. TensorRT μµμ ν™”

#### κ°λ…

NVIDIAμ μ¶”λ΅  μ—”μ§„μΌλ΅ λ¨λΈμ„ μµμ ν™”ν•μ—¬ μ¶”λ΅  μ†λ„ ν–¥μƒ

#### μ μ© κ°€λ¥μ„±: β­β­β­β­ (λ†’μ)

#### μ¥μ 

1. **κ·Ήλ„μ μ„±λ¥ ν–¥μƒ**
   - INT8 μ–‘μν™”λ΅ 3-10λ°° μ†λ„ ν–¥μƒ
   - FP16 μ–‘μν™”λ΅ 2-5λ°° μ†λ„ ν–¥μƒ
   - GPU ν™μ©λ¥  κ·Ήλ€ν™”

2. **λ©”λ¨λ¦¬ ν¨μ¨μ„±**
   - μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ°μ†
   - λ” ν° λ°°μΉ ν¬κΈ° κ°€λ¥

3. **ν”„λ΅λ•μ… μµμ ν™”**
   - μ•μ •μ μΈ μ¶”λ΅  μ„±λ¥
   - μ§€μ—° μ‹κ°„ μµμ†ν™”

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **κµ¬ν„ λ³µμ΅λ„**
   - ONNX λ³€ν™ ν•„μ”
   - TensorRT μ—”μ§„ μƒμ„± μ‹κ°„
   - λ°°μΉ ν¬κΈ° κ³ μ • μ‹ μ μ—°μ„± κ°μ†

2. **ν•λ“μ›¨μ–΄ μ μ•½**
   - NVIDIA GPUλ§ μ§€μ›
   - μµμ‹  GPUμ—μ„ μµμ  μ„±λ¥

3. **λ¨λΈ νΈν™μ„±**
   - λ¨λ“  λ¨λΈμ΄ TensorRT μ§€μ›ν•μ§€ μ•μ
   - μ»¤μ¤ν…€ λ μ΄μ–΄ μ²λ¦¬ ν•„μ”

#### ν„μ¬ ν”„λ΅μ νΈ μ μ© μ‹λ‚λ¦¬μ¤

```python
# TensorRT μ—”μ§„ μƒμ„±
import torch
import tensorrt as trt

# 1. PyTorch λ¨λΈμ„ ONNXλ΅ λ³€ν™
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=13,
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch"}, "output": {0: "batch"}}
)

# 2. TensorRT μ—”μ§„ μƒμ„± (trtexec μ‚¬μ©)
# trtexec --onnx=model.onnx --saveEngine=model.trt --fp16 --workspace=4096

# 3. TensorRT μ—”μ§„ λ΅λ“ λ° μ¶”λ΅ 
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

# μ—”μ§„ λ΅λ“ λ° μ¶”λ΅ 
```

#### κ¶μ¥μ‚¬ν•­

- **λ‹¨κΈ°**: μ„ νƒμ  μ μ© (λ³µμ΅λ„ λ€λΉ„)
- **μ¤‘κΈ°**: ν”„λ΅λ•μ… μµμ ν™” ν•„μ” μ‹
- **μ¥κΈ°**: λ€κ·λ¨ μ„λΉ™ ν™κ²½μ—μ„ ν•„μ

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: λ†’μ
- **μ„±λ¥ ν–¥μƒ**: λ§¤μ° λ†’μ (3-10λ°° μ†λ„ ν–¥μƒ)
- **λΉ„μ© μ κ°**: μ¤‘κ°„
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­ (λ†’μ, ν”„λ΅λ•μ… μµμ ν™” μ‹)

---

### 10. μ‚¬μ „ κ²°κ³Ό μΊμ‹± (Pre-computation Caching)

#### κ°λ…

μμ£Ό μ”μ²­λλ” μ…λ ¥μ— λ€ν• κ²°κ³Όλ¥Ό λ―Έλ¦¬ κ³„μ‚°ν•μ—¬ μ €μ¥

#### μ μ© κ°€λ¥μ„±: β­β­β­β­β­ (λ§¤μ° λ†’μ)

#### μΊμ‹± μ „λµ

**1. μΈκΈ° λ μ¤ν† λ‘ λ¦¬λ·° λ¶„μ„ κ²°κ³Ό μΊμ‹±**

```python
# μΈκΈ° λ μ¤ν† λ‘μ κ°μ„± λ¶„μ„ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°
class PrecomputedSentimentCache:
    def __init__(self, sentiment_analyzer, redis_client=None):
        self.analyzer = sentiment_analyzer
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
    
    def precompute_popular_restaurants(self, restaurant_ids, review_data):
        """μΈκΈ° λ μ¤ν† λ‘μ κ°μ„± λ¶„μ„ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°"""
        for restaurant_id in restaurant_ids:
            reviews = review_data.get(restaurant_id, [])
            if reviews:
                result = self.analyzer.analyze(
                    review_list=[r["review"] for r in reviews],
                    restaurant_name=reviews[0]["restaurant_name"],
                    restaurant_id=restaurant_id
                )
                # μΊμ‹ μ €μ¥ (TTL: 24μ‹κ°„)
                cache_key = f"sentiment:{restaurant_id}"
                self.redis.setex(
                    cache_key,
                    86400,
                    json.dumps(result)
                )
    
    def get_cached_result(self, restaurant_id):
        """μΊμ‹λ κ²°κ³Ό μ΅°ν"""
        cache_key = f"sentiment:{restaurant_id}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None
```

**2. μμ£Ό κ²€μƒ‰λλ” μΏΌλ¦¬ κ²°κ³Ό μΊμ‹±**

```python
# μμ£Ό κ²€μƒ‰λλ” λ²΅ν„° κ²€μƒ‰ κ²°κ³Ό μΊμ‹±
class PrecomputedVectorCache:
    def __init__(self, vector_search, redis_client=None):
        self.vector_search = vector_search
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.popular_queries = [
            "λ§›μλ‹¤",
            "μΆ‹λ‹¤",
            "λ§μ΅±",
            "λ³„λ΅",
            "λ¶λ§",
            "μ¶”μ²",
        ]
    
    def precompute_popular_queries(self):
        """μΈκΈ° κ²€μƒ‰ μΏΌλ¦¬ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°"""
        for query in self.popular_queries:
            results = self.vector_search.query_similar_reviews(
                query_text=query,
                limit=50
            )
            cache_key = f"vector_search:{query}"
            self.redis.setex(
                cache_key,
                3600,  # 1μ‹κ°„
                json.dumps(results)
            )
    
    def get_cached_search(self, query_text):
        """μΊμ‹λ κ²€μƒ‰ κ²°κ³Ό μ΅°ν"""
        cache_key = f"vector_search:{query_text}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None
```

**3. λ°°μΉ μ‘μ—… μ¤μΌ€μ¤„λ§**

```python
# μ£ΌκΈ°μ μΌλ΅ μΈκΈ° λ°μ΄ν„° μ‚¬μ „ κ³„μ‚°
import schedule
import time

def precompute_daily():
    """λ§¤μΌ μΈκΈ° λ μ¤ν† λ‘ λ¶„μ„ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°"""
    # μΈκΈ° λ μ¤ν† λ‘ ID λ¦¬μ¤νΈ κ°€μ Έμ¤κΈ°
    popular_restaurants = get_popular_restaurants()
    
    # κ°μ„± λ¶„μ„ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°
    sentiment_cache = PrecomputedSentimentCache(sentiment_analyzer)
    sentiment_cache.precompute_popular_restaurants(
        popular_restaurants,
        review_data
    )
    
    # λ²΅ν„° κ²€μƒ‰ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°
    vector_cache = PrecomputedVectorCache(vector_search)
    vector_cache.precompute_popular_queries()

# λ§¤μΌ μƒλ²½ 2μ‹μ— μ‹¤ν–‰
schedule.every().day.at("02:00").do(precompute_daily)

# λ°±κ·ΈλΌμ΄λ“μ—μ„ μ‹¤ν–‰
while True:
    schedule.run_pending()
    time.sleep(60)
```

#### μ¥μ 

1. **μ‘λ‹µ μ‹κ°„ λ‹¨μ¶•**
   - μ‚¬μ „ κ³„μ‚°λ κ²°κ³Ό μ¦‰μ‹ λ°ν™
   - μ‚¬μ©μ κ²½ν— ν–¥μƒ

2. **λΉ„μ© μ κ°**
   - LLM API νΈμ¶ κ°μ†
   - GPU μ‚¬μ©λ‰ μµμ ν™”

3. **λ¶€ν• λ¶„μ‚°**
   - ν”Όν¬ μ‹κ°„ λ¶€ν• κ°μ†
   - μ•μ •μ μΈ μ„λΉ„μ¤ μ κ³µ

#### λ‹¨μ  λ° κ³ λ ¤μ‚¬ν•­

1. **λ°μ΄ν„° μ‹ μ„ λ„**
   - μΊμ‹ λ¬΄ν¨ν™” μ „λµ ν•„μ”
   - μ‹¤μ‹κ°„ λ°μ΄ν„° μ—…λ°μ΄νΈ μ²λ¦¬

2. **μ €μ¥ κ³µκ°„**
   - μΊμ‹ μ €μ¥ κ³µκ°„ ν•„μ”
   - λ©”λ¨λ¦¬ κ΄€λ¦¬ ν•„μ”

3. **μμΈ΅ μ •ν™•λ„**
   - μΈκΈ° λ°μ΄ν„° μμΈ΅ ν•„μ”
   - μ‚¬μ© ν¨ν„΄ λ¶„μ„ ν•„μ”

#### κ¶μ¥μ‚¬ν•­

- **μ¦‰μ‹ μ μ©**: μΈκΈ° λ μ¤ν† λ‘ κ²°κ³Ό μΊμ‹±
- **λ‹¨κΈ° μ μ©**: μμ£Ό κ²€μƒ‰λλ” μΏΌλ¦¬ μΊμ‹±
- **μ¤‘κΈ° μ μ©**: λ°°μΉ μ‘μ—… μ¤μΌ€μ¤„λ§

#### ROI ν‰κ°€

- **κµ¬ν„ λ³µμ΅λ„**: μ¤‘κ°„
- **μ„±λ¥ ν–¥μƒ**: λ§¤μ° λ†’μ (10-100λ°° μ†λ„ ν–¥μƒ κ°€λ¥)
- **λΉ„μ© μ κ°**: λ§¤μ° λ†’μ (API νΈμ¶ λ€ν­ κ°μ†)
- **μΆ…ν•© ν‰κ°€**: β­β­β­β­β­ (λ§¤μ° λ†’μ)

---

## ν†µν•© μ μ© μ „λµ

### Phase 1: μ¦‰μ‹ μ μ© (1-2μ£Ό) β΅

#### μ°μ„ μμ„ 1: CUDA μµμ ν™”
- Transformers Pipeline GPU μ‚¬μ©
- SentenceTransformer GPU μ‚¬μ©
- λ°°μΉ ν¬κΈ° μµμ ν™”

#### μ°μ„ μμ„ 2: μ–‘μν™” (FP16)
- Transformers Pipeline FP16 μ–‘μν™”
- SentenceTransformer FP16 μ–‘μν™”

#### μ°μ„ μμ„ 3: κ²°κ³Ό μΊμ‹±
- Redisλ¥Ό μ‚¬μ©ν• κ²°κ³Ό μΊμ‹±
- LLM μ‘λ‹µ μΊμ‹±
- μ„λ² λ”© κ²°κ³Ό μΊμ‹±

**μμƒ ν¨κ³Ό:**
- μ†λ„: 2-5λ°° ν–¥μƒ
- λΉ„μ©: API νΈμ¶ 30-50% κ°μ†
- κµ¬ν„ λ³µμ΅λ„: λ‚®μ
- ROI: β­β­β­β­β­

---

### Phase 2: λ‹¨κΈ° μ μ© (1-2κ°μ›”) π€

#### μ°μ„ μμ„ 1: vLLM λ„μ…
- λ΅μ»¬ LLM μ„λ²„ κµ¬μ¶•
- Qwen2.5-7B-Instruct μ‚¬μ©
- OpenAI API μ μ§„μ  λ€μ²΄

#### μ°μ„ μμ„ 2: μ‚¬μ „ κ²°κ³Ό μΊμ‹±
- μΈκΈ° λ μ¤ν† λ‘ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°
- μμ£Ό κ²€μƒ‰λλ” μΏΌλ¦¬ κ²°κ³Ό μΊμ‹±
- λ°°μΉ μ‘μ—… μ¤μΌ€μ¤„λ§

#### μ°μ„ μμ„ 3: λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ •
- GPU λ©”λ¨λ¦¬μ— λ§λ” λ°°μΉ ν¬κΈ° μλ™ μ΅°μ •
- λ°°μΉ λ€κΈ° μ‹κ°„ μµμ ν™”

**μμƒ ν¨κ³Ό:**
- μ†λ„: 5-10λ°° ν–¥μƒ
- λΉ„μ©: API λΉ„μ© 70-90% κ°μ†
- κµ¬ν„ λ³µμ΅λ„: μ¤‘κ°„
- ROI: β­β­β­β­β­

---

### Phase 3: μ¤‘κΈ° μ μ© (3-6κ°μ›”) π“

#### μ°μ„ μμ„ 1: TensorRT μµμ ν™”
- SentenceTransformer TensorRT λ³€ν™
- INT8 μ–‘μν™” μ μ©
- λ°°μΉ ν¬κΈ° κ³ μ • μµμ ν™”

#### μ°μ„ μμ„ 2: QLoRA νμΈνλ‹ (μ‘μ—…λ³„ λ³„λ„ λ¨λΈ)
- λ°μ΄ν„°μ…‹ κµ¬μ¶•
- κ°μ„± λ¶„μ„ μ¬λ¶„λ¥ λ¨λΈ νμΈνλ‹
- μ”μ•½/κ°•μ  μ¶”μ¶ λ¨λΈ νμΈνλ‹
- μ„±λ¥ λΉ„κµ λ° κ²€μ¦
- **μ£Όμ**: MoE λ€μ‹  μ‘μ—…λ³„ λ³„λ„ λ¨λΈ κ¶μ¥

#### μ°μ„ μμ„ 3: κ³ κΈ‰ μΊμ‹± μ „λµ
- λ¶€λ¶„ μΊμ‹± (Partial Caching)
- κ³„μΈµμ  μΊμ‹± (Hierarchical Caching)
- μ§€λ¥ν• μΊμ‹ λ¬΄ν¨ν™”

**μμƒ ν¨κ³Ό:**
- μ†λ„: 3-10λ°° μ¶”κ°€ ν–¥μƒ
- λΉ„μ©: μ¶”κ°€ 20-30% μ κ°
- κµ¬ν„ λ³µμ΅λ„: λ†’μ
- ROI: β­β­β­β­

---

### Phase 4: μ¥κΈ° κ²€ν†  (6κ°μ›”+) π”¬

#### μ°μ„ μμ„ 1: LoRA/QLoRA ν™•μ¥
- μ”μ•½/κ°•μ  μ¶”μ¶ λ¨λΈ νμΈνλ‹
- λ©€ν‹°νƒμ¤ν¬ λ¨λΈ κµ¬μ¶•
- vLLMκ³Ό ν†µν•©

#### μ°μ„ μμ„ 2: μ§€μ‹μ¦λ¥ (μ„ νƒμ )
- νΉμ • μ‘μ—…μ—λ§ μ μ©
- λ¨λΈ ν¬κΈ° μµμ ν™”

#### μ°μ„ μμ„ 3: MoE νμΈνλ‹ (μµν›„μ μλ‹¨, λΉ„κ¶μ¥)
- **μ£Όμ**: μ‘μ—…λ³„ λ³„λ„ λ¨λΈ(QLoRA)μ΄ λ” μ‹¤μ©μ 
- λ¨λ“  λ‹¤λ¥Έ λ°©λ²•μ΄ μ‹¤ν¨ν–μ„ λ•λ§ κ²€ν† 
- λ§¤μ° νΉμν• μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§
- μ¶©λ¶„ν• λ°μ΄ν„°μ™€ λ¦¬μ†μ¤ ν™•λ³΄ ν›„
- MoE νμΈνλ‹ μ „λ¬Έ μ§€μ‹ ν•„μ”

**μμƒ ν¨κ³Ό:**
- μ†λ„: μ¶”κ°€ 2-3λ°° ν–¥μƒ (μ‘μ—…λ³„ λ³„λ„ λ¨λΈκ³Ό μ μ‚¬)
- λΉ„μ©: μ¶”κ°€ 10-20% μ κ° (ν•™μµ λΉ„μ©μ΄ νΌ)
- κµ¬ν„ λ³µμ΅λ„: λ§¤μ° λ†’μ
- ROI: β­β­ (λ‚®μ, λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤)

---

## κµ¬ν„ κ°€μ΄λ“

### 1. CUDA + μ–‘μν™” ν†µν•© κµ¬ν„

```python
# src/config.py
import os
import torch

class Config:
    # GPU μ„¤μ •
    USE_GPU: bool = os.getenv("USE_GPU", "true").lower() == "true"
    GPU_DEVICE: int = int(os.getenv("GPU_DEVICE", "0"))
    USE_FP16: bool = os.getenv("USE_FP16", "true").lower() == "true"
    
    @classmethod
    def get_device(cls):
        """GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ"""
        if cls.USE_GPU and torch.cuda.is_available():
            return cls.GPU_DEVICE
        return -1
    
    @classmethod
    def get_dtype(cls):
        """μ–‘μν™” νƒ€μ… λ°ν™"""
        if cls.USE_FP16 and torch.cuda.is_available():
            return torch.float16
        return torch.float32
```

```python
# src/sentiment_analysis.py
from .config import Config

class SentimentAnalyzer:
    def __init__(self, ...):
        device = Config.get_device()
        dtype = Config.get_dtype()
        
        self.sentiment = pipeline(
            "sentiment-analysis",
            model=model_name,
            tokenizer=model_name,
            device=device,
            torch_dtype=dtype,  # FP16 μ–‘μν™”
            batch_size=64 if device >= 0 else 32,
        )
```

```python
# src/vector_search.py
from .config import Config
import torch

class VectorSearch:
    def __init__(self, ...):
        self.encoder = SentenceTransformer(Config.EMBEDDING_MODEL)
        
        if Config.USE_GPU and torch.cuda.is_available():
            self.encoder = self.encoder.cuda()
            if Config.USE_FP16:
                self.encoder = self.encoder.half()  # FP16 μ–‘μν™”
            self.batch_size = 64
        else:
            self.batch_size = 32
```

---

### 2. μΊμ‹± μ‹μ¤ν… ν†µν•© κµ¬ν„

```python
# src/cache.py
import redis
import hashlib
import json
from typing import Optional, Any
import logging

logger = logging.getLogger(__name__)

class CacheManager:
    """ν†µν•© μΊμ‹± κ΄€λ¦¬μ"""
    
    def __init__(self, redis_host='localhost', redis_port=6379, redis_db=0):
        try:
            self.redis = redis.Redis(
                host=redis_host,
                port=redis_port,
                db=redis_db,
                decode_responses=False
            )
            self.redis.ping()  # μ—°κ²° ν…μ¤νΈ
            self.enabled = True
        except Exception as e:
            logger.warning(f"Redis μ—°κ²° μ‹¤ν¨: {e}. μΊμ‹± λΉ„ν™μ„±ν™”.")
            self.redis = None
            self.enabled = False
    
    def _get_key(self, prefix: str, content: str) -> str:
        """μΊμ‹ ν‚¤ μƒμ„±"""
        hash_content = hashlib.md5(content.encode()).hexdigest()
        return f"{prefix}:{hash_content}"
    
    def get(self, prefix: str, content: str) -> Optional[Any]:
        """μΊμ‹ μ΅°ν"""
        if not self.enabled:
            return None
        
        try:
            key = self._get_key(prefix, content)
            cached = self.redis.get(key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.error(f"μΊμ‹ μ΅°ν μ‹¤ν¨: {e}")
        
        return None
    
    def set(self, prefix: str, content: str, value: Any, ttl: int = 3600):
        """μΊμ‹ μ €μ¥"""
        if not self.enabled:
            return
        
        try:
            key = self._get_key(prefix, content)
            self.redis.setex(key, ttl, json.dumps(value))
        except Exception as e:
            logger.error(f"μΊμ‹ μ €μ¥ μ‹¤ν¨: {e}")
    
    def delete(self, prefix: str, content: str):
        """μΊμ‹ μ‚­μ """
        if not self.enabled:
            return
        
        try:
            key = self._get_key(prefix, content)
            self.redis.delete(key)
        except Exception as e:
            logger.error(f"μΊμ‹ μ‚­μ  μ‹¤ν¨: {e}")
```

```python
# src/llm_utils.py μμ •
from .cache import CacheManager

class LLMUtils:
    def __init__(self, openai_client, model, cache_manager=None):
        self.client = openai_client
        self.model = model
        self.cache = cache_manager or CacheManager()
    
    def summarize_reviews(self, positive_reviews, negative_reviews):
        # μΊμ‹ ν‚¤ μƒμ„±
        cache_content = json.dumps({
            "positive": positive_reviews,
            "negative": negative_reviews
        }, sort_keys=True)
        
        # μΊμ‹ ν™•μΈ
        cached = self.cache.get("llm_summarize", cache_content)
        if cached:
            logger.info("μΊμ‹μ—μ„ κ²°κ³Ό λ°ν™")
            return cached
        
        # LLM νΈμ¶
        response = self.client.chat.completions.create(...)
        result = json.loads(response.choices[0].message.content)
        
        # μΊμ‹ μ €μ¥ (1μ‹κ°„)
        self.cache.set("llm_summarize", cache_content, result, ttl=3600)
        
        return result
```

---

### 3. vLLM ν†µν•© κµ¬ν„

```python
# vllm_server.py
from vllm import LLM, SamplingParams
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import logging

logger = logging.getLogger(__name__)

# λ¨λΈ λ΅λ“
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

sampling_params = SamplingParams(
    temperature=0.3,
    top_p=0.95,
    max_tokens=1024,
)

app = FastAPI()

class GenerateRequest(BaseModel):
    prompt: str
    temperature: float = 0.3
    max_tokens: int = 1024

@app.post("/generate")
async def generate(request: GenerateRequest):
    try:
        params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_tokens,
        )
        outputs = llm.generate([request.prompt], params)
        return {"text": outputs[0].outputs[0].text}
    except Exception as e:
        logger.error(f"vLLM μ¶”λ΅  μ‹¤ν¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

```python
# src/llm_utils.pyμ— vLLM μµμ… μ¶”κ°€
import requests
from typing import Optional

class LLMUtils:
    def __init__(
        self,
        openai_client: Optional[OpenAI] = None,
        use_vllm: bool = False,
        vllm_url: Optional[str] = None,
    ):
        self.use_vllm = use_vllm
        if use_vllm:
            self.vllm_url = vllm_url or "http://localhost:8001"
        else:
            self.client = openai_client or OpenAI()
    
    def _call_vllm(self, prompt: str, temperature: float = 0.3) -> str:
        """vLLM μ„λ²„ νΈμ¶"""
        try:
            response = requests.post(
                f"{self.vllm_url}/generate",
                json={"prompt": prompt, "temperature": temperature},
                timeout=30
            )
            response.raise_for_status()
            return response.json()["text"]
        except Exception as e:
            logger.error(f"vLLM νΈμ¶ μ‹¤ν¨: {e}")
            raise
    
    def summarize_reviews(self, ...):
        prompt = self._build_summarize_prompt(...)
        
        if self.use_vllm:
            response_text = self._call_vllm(prompt, temperature=0.3)
        else:
            response = self.client.chat.completions.create(...)
            response_text = response.choices[0].message.content
        
        return json.loads(response_text)
```

---

## λΉ„μ©-ν¨κ³Ό λ¶„μ„

### κΈ°μ λ³„ λΉ„κµν‘

| κΈ°μ  | κµ¬ν„ λ³µμ΅λ„ | μ„±λ¥ ν–¥μƒ | λΉ„μ© μ κ° | ROI | μ°μ„ μμ„ |
|------|------------|----------|----------|-----|---------|
| **CUDA μµμ ν™”** | β­ | β­β­β­β­ | β­β­β­ | β­β­β­β­β­ | 1 |
| **μ–‘μν™” (FP16)** | β­ | β­β­β­β­ | β­β­β­ | β­β­β­β­β­ | 2 |
| **κ²°κ³Ό μΊμ‹±** | β­β­ | β­β­β­β­β­ | β­β­β­β­β­ | β­β­β­β­β­ | 3 |
| **μ‚¬μ „ κ²°κ³Ό μΊμ‹±** | β­β­β­ | β­β­β­β­β­ | β­β­β­β­β­ | β­β­β­β­β­ | 4 |
| **vLLM** | β­β­β­ | β­β­β­β­β­ | β­β­β­β­ | β­β­β­β­β­ | 5 |
| **λ°°μΉ μ²λ¦¬ μµμ ν™”** | β­ | β­β­β­ | β­β­ | β­β­β­β­ | 6 |
| **QLoRA** | β­β­β­β­ | β­β­β­ | β­β­β­ | β­β­β­β­ | 7 |
| **TensorRT** | β­β­β­β­ | β­β­β­β­β­ | β­β­β­ | β­β­β­β­ | 8 |
| **LoRA** | β­β­β­ | β­β­β­ | β­β­β­ | β­β­β­ | 9 |
| **MoE νμΈνλ‹** | β­β­β­β­β­ | β­β­ | β­ | β­β­ | 10 (λΉ„κ¶μ¥) |
| **μ§€μ‹μ¦λ¥** | β­β­β­β­β­ | β­β­ | β­β­ | β­β­ | 11 |

### λ„μ  ν¨κ³Ό μμƒ

#### Phase 1 μ μ© ν›„
- **μ†λ„ ν–¥μƒ**: 2-5λ°°
- **λΉ„μ© μ κ°**: 30-50%
- **κµ¬ν„ μ‹κ°„**: 1-2μ£Ό

#### Phase 2 μ μ© ν›„
- **μ†λ„ ν–¥μƒ**: 10-20λ°°
- **λΉ„μ© μ κ°**: 70-90%
- **κµ¬ν„ μ‹κ°„**: 1-2κ°μ›”

#### Phase 3 μ μ© ν›„
- **μ†λ„ ν–¥μƒ**: 20-50λ°°
- **λΉ„μ© μ κ°**: 85-95%
- **κµ¬ν„ μ‹κ°„**: 3-6κ°μ›”

---

## μµμΆ… κ¶μ¥μ‚¬ν•­

### μ¦‰μ‹ μ μ© (1μ£Ό λ‚΄) β΅

1. **CUDA μµμ ν™”** β…
   - Transformers Pipeline GPU μ‚¬μ©
   - SentenceTransformer GPU μ‚¬μ©
   - λ°°μΉ ν¬κΈ° μµμ ν™”

2. **μ–‘μν™” (FP16)** β…
   - FP16 μ–‘μν™” μ μ©
   - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ λ°μΌλ΅ κ°μ†

3. **κ²°κ³Ό μΊμ‹±** β…
   - Redisλ¥Ό μ‚¬μ©ν• κ²°κ³Ό μΊμ‹±
   - LLM μ‘λ‹µ μΊμ‹±

**μμƒ ν¨κ³Ό:**
- μ†λ„: 2-5λ°° ν–¥μƒ
- λΉ„μ©: 30-50% μ κ°
- ROI: β­β­β­β­β­

---

### λ‹¨κΈ° μ μ© (1-2κ°μ›”) π€

4. **vLLM λ„μ…** β…
   - λ΅μ»¬ LLM μ„λ²„ κµ¬μ¶•
   - OpenAI API μ μ§„μ  λ€μ²΄

5. **μ‚¬μ „ κ²°κ³Ό μΊμ‹±** β…
   - μΈκΈ° λ μ¤ν† λ‘ κ²°κ³Ό μ‚¬μ „ κ³„μ‚°
   - λ°°μΉ μ‘μ—… μ¤μΌ€μ¤„λ§

6. **λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ •** β…
   - GPU λ©”λ¨λ¦¬μ— λ§λ” λ°°μΉ ν¬κΈ° μλ™ μ΅°μ •

**μμƒ ν¨κ³Ό:**
- μ†λ„: 10-20λ°° ν–¥μƒ
- λΉ„μ©: 70-90% μ κ°
- ROI: β­β­β­β­β­

---

### μ¤‘κΈ° μ μ© (3-6κ°μ›”) π“

7. **TensorRT μµμ ν™”** β οΈ
   - SentenceTransformer TensorRT λ³€ν™
   - ν”„λ΅λ•μ… μµμ ν™” ν•„μ” μ‹

8. **QLoRA νμΈνλ‹** β οΈ
   - λ°μ΄ν„°μ…‹ ν™•λ³΄ ν›„
   - λ„λ©”μΈ νΉν™” ν•„μ” μ‹

**μμƒ ν¨κ³Ό:**
- μ†λ„: μ¶”κ°€ 3-10λ°° ν–¥μƒ
- λΉ„μ©: μ¶”κ°€ 20-30% μ κ°
- ROI: β­β­β­β­

---

### μ¥κΈ° κ²€ν†  (6κ°μ›”+) π”¬

9. **LoRA/QLoRA ν™•μ¥** β οΈ
   - λ©€ν‹°νƒμ¤ν¬ λ¨λΈ κµ¬μ¶•
   - νΉμ μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§

10. **MoE νμΈνλ‹** β
    - **λΉ„κ¶μ¥**: μ‘μ—…λ³„ λ³„λ„ λ¨λΈ(QLoRA)μ΄ λ” μ‹¤μ©μ 
    - λ¨λ“  λ‹¤λ¥Έ λ°©λ²•μ΄ μ‹¤ν¨ν–μ„ λ•λ§ κ²€ν† 
    - λ§¤μ° νΉμν• μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§
    - λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤

11. **μ§€μ‹μ¦λ¥** β
    - νΉμ • μ‘μ—…μ—λ§ μ„ νƒμ  μ μ©
    - λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤

**μμƒ ν¨κ³Ό:**
- μ†λ„: μ¶”κ°€ 2-3λ°° ν–¥μƒ
- λΉ„μ©: μ¶”κ°€ 10-20% μ κ° (MoEλ” ν•™μµ λΉ„μ© νΌ)
- ROI: β­β­ (λ‚®μ)

---

## κ²°λ΅ 

### ν•µμ‹¬ κ¶μ¥μ‚¬ν•­

1. **μ¦‰μ‹ μ μ©**: CUDA + μ–‘μν™” + μΊμ‹± (λ†’μ€ ROI, λ‚®μ€ λ³µμ΅λ„)
2. **λ‹¨κΈ° μ μ©**: vLLM + μ‚¬μ „ κ²°κ³Ό μΊμ‹± (νΈλν”½ μ¦κ°€ μ‹)
3. **μ¤‘κΈ° μ μ©**: TensorRT + QLoRA (ν”„λ΅λ•μ… μµμ ν™” ν•„μ” μ‹)
   - **μ‘μ—…λ³„ λ³„λ„ λ¨λΈ** κ¶μ¥ (MoE λ€μ‹ )
4. **μ¥κΈ° κ²€ν† **: LoRA ν™•μ¥ (νΉμ μ”κµ¬μ‚¬ν•­ μμ„ λ•λ§)
5. **μµν›„μ μλ‹¨**: MoE νμΈνλ‹ (λΉ„κ¶μ¥, λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤)

### μ°μ„ μμ„ μ”μ•½

**μµμ°μ„  (μ¦‰μ‹):**
- CUDA μµμ ν™”
- μ–‘μν™” (FP16)
- κ²°κ³Ό μΊμ‹±

**λ†’μ€ μ°μ„ μμ„ (λ‹¨κΈ°):**
- vLLM λ„μ…
- μ‚¬μ „ κ²°κ³Ό μΊμ‹±
- λ™μ  λ°°μΉ ν¬κΈ° μ΅°μ •

**μ¤‘κ°„ μ°μ„ μμ„ (μ¤‘κΈ°):**
- TensorRT μµμ ν™”
- QLoRA νμΈνλ‹ (μ‘μ—…λ³„ λ³„λ„ λ¨λΈ κ¶μ¥)

**λ‚®μ€ μ°μ„ μμ„ (μ¥κΈ°):**
- LoRA ν™•μ¥
- μ§€μ‹μ¦λ¥

**μµν›„μ μλ‹¨ (λ§¤μ° νΉμν• κ²½μ°):**
- MoE νμΈνλ‹ (λΉ„κ¶μ¥, λ³µμ΅λ„ λ€λΉ„ ν¨κ³Ό λ¶ν™•μ‹¤)

### μμƒ μµμΆ… μ„±λ¥

λ¨λ“  μµμ ν™” μ μ© ν›„:
- **μ†λ„ ν–¥μƒ**: 20-50λ°°
- **λΉ„μ© μ κ°**: 85-95%
- **μ‘λ‹µ μ‹κ°„**: 100ms μ΄ν• (μΊμ‹ ννΈ μ‹)
- **λ™μ‹ μ²λ¦¬λ‰**: 10λ°° μ΄μƒ μ¦κ°€

---

**μ‘μ„±μ**: AI Assistant  
**μµμΆ… μμ •μΌ**: 2026λ…„ 1μ›”  
**λ²„μ „**: 1.0.0

